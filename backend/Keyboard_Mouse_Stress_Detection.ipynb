{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejrqLp58QuVj",
        "outputId": "c816f583-3f24-4d1f-9f32-2eb2cf55312e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-75323e3c-b732-86a5-7635-412da39d837a)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-polars-cu12 25.6.0 requires polars<1.29,>=1.25, but you have polars 1.4.1 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.9/780.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m123.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m130.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m128.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m123.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# ---- 0) Setup & Imports\n",
        "!nvidia-smi -L || true\n",
        "import os, sys, json, math, random, io, zipfile, glob, gc, shutil, time\n",
        "from pathlib import Path\n",
        "\n",
        "# Fast IO\n",
        "!pip -q install polars==1.4.1 pyarrow==16.1.0 datasets==2.20.0\n",
        "import polars as pl\n",
        "\n",
        "# Torch\n",
        "!pip -q install torch==2.3.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Core\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "biuim2YhQ3aV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 1) Paths\n",
        "DRIVE_ROOT = Path('/content/drive/MyDrive/mindease_behavior')\n",
        "RAW = DRIVE_ROOT/'raw'\n",
        "INTER = DRIVE_ROOT/'intermediate'\n",
        "ART = DRIVE_ROOT/'artifacts'\n",
        "# for p in [RAW, INTER, ART]: p.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "l8RzKDzmQ-Xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AALTO_ZIP = RAW/'aalto.zip'\n",
        "BALABIT_DIR = RAW/'balabit'"
      ],
      "metadata": {
        "id": "wCpBWYiEdEzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 2) Download datasets\n",
        "\n",
        "# Aalto 136M keystrokes: direct \"Data\" link (zip ~1.4GB). If this fails, visit the page and right-click copy link.\n",
        "# We pull the anchor the page points to; if the URL changes, open the page and copy the \"Text files of the keystrokes in a .zip\".\n",
        "AALTO_ZIP = RAW/'aalto.zip'\n",
        "if not AALTO_ZIP.exists():\n",
        "    # Attempt to fetch the target zip linked on the page. The link resolves under the same domain.\n",
        "    # If you get a 403 due to hotlinking, open the page, copy the zip URL, and paste here.\n",
        "    !wget -O \"{AALTO_ZIP}\" \"https://userinterfaces.aalto.fi/136Mkeystrokes/data/Keystrokes.zip\"\n",
        "\n",
        "# Balabit mouse dynamics: clone repository (small)\n",
        "BALABIT_DIR = RAW/'balabit'\n",
        "if not BALABIT_DIR.exists():\n",
        "    !git clone https://github.com/balabit/Mouse-Dynamics-Challenge \"{BALABIT_DIR}\"\n",
        "\n",
        "print(\"Downloads present:\")\n",
        "print(\"Aalto zip exists:\", AALTO_ZIP.exists(), AALTO_ZIP.stat().st_size if AALTO_ZIP.exists() else 0)\n",
        "print(\"Balabit folder exists:\", BALABIT_DIR.exists())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXb4hPn4Ro8v",
        "outputId": "4671a9e3-89bb-453f-df1d-72d91df0fb3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/mindease_behavior/raw/aalto_136m.zip: No such file or directory\n",
            "Cloning into '/content/drive/MyDrive/mindease_behavior/raw/balabit'...\n",
            "remote: Enumerating objects: 1711, done.\u001b[K\n",
            "remote: Total 1711 (delta 0), reused 0 (delta 0), pack-reused 1711 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1711/1711), 42.60 MiB | 11.68 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "Updating files: 100% (1678/1678), done.\n",
            "Downloads present:\n",
            "Aalto zip exists: False 0\n",
            "Balabit folder exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AALTO_DIR = RAW/'aalto'"
      ],
      "metadata": {
        "id": "1l_z9mZ9wmpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 3) Unpack Aalto zip (large!) → to RAW/aalto/\n",
        "AALTO_DIR = RAW/'aalto'\n",
        "if not AALTO_DIR.exists():\n",
        "    AALTO_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    with zipfile.ZipFile(AALTO_ZIP, 'r') as zf:\n",
        "        zf.extractall(AALTO_DIR)"
      ],
      "metadata": {
        "id": "x9_207pIciXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 4) Preprocessing configs\n",
        "CFG = {\n",
        "    \"keystroke\": {\n",
        "        \"window_sec\": 30,\n",
        "        \"stride_sec\": 15,\n",
        "        \"max_users\": 30,        # reduce if RAM/bandwidth tight; increase if you can\n",
        "        \"max_sessions_per_user\": 6,  # cap per-user sessions from Aalto\n",
        "        \"min_events_per_window\": 20  # skip super-sparse windows\n",
        "    },\n",
        "    \"mouse\": {\n",
        "        \"window_sec\": 30,\n",
        "        \"stride_sec\": 15,\n",
        "        \"min_events_per_window\": 30\n",
        "    },\n",
        "    \"embedding_dim\": 96,\n",
        "    \"batch_size\": 256,\n",
        "    \"epochs\": 6,               # small for a one-week sprint\n",
        "    \"lr\": 2e-3,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "}\n",
        "with open(ART/'config.json', 'w') as f:\n",
        "    json.dump(CFG, f, indent=2)\n",
        "print(\"CFG:\", CFG)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC8jwAqlvr-s",
        "outputId": "6deae367-c8d7-45ea-8c42-ab23b1e1d53e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CFG: {'keystroke': {'window_sec': 30, 'stride_sec': 15, 'max_users': 30, 'max_sessions_per_user': 6, 'min_events_per_window': 20}, 'mouse': {'window_sec': 30, 'stride_sec': 15, 'min_events_per_window': 30}, 'embedding_dim': 96, 'batch_size': 256, 'epochs': 6, 'lr': 0.002, 'device': 'cuda'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OUT_DIR = INTER\n",
        "BLEND_LIST = INTER / \"keystroke_sample_files_blend.txt\"\n",
        "SIZE_LIST   = INTER/'keystroke_sample_files_by_size.txt'"
      ],
      "metadata": {
        "id": "oIRmxtscBCBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if BLEND_LIST.exists():\n",
        "    SAMPLE_LIST = BLEND_LIST\n",
        "elif SIZE_LIST.exists():\n",
        "    SAMPLE_LIST = SIZE_LIST\n",
        "else:\n",
        "    raise SystemExit(\"No sample list found in INTER. Expected one of:\\n\"\n",
        "                     f\"- {BLEND_LIST}\\n- {SIZE_LIST}\")\n",
        "\n",
        "with open(SAMPLE_LIST, 'r') as f:\n",
        "    aalto_files = [ln.strip() for ln in f if ln.strip()]\n",
        "\n",
        "print(f\"Using sample list: {SAMPLE_LIST.name}  |  files: {len(aalto_files):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MDmxsgyKYil",
        "outputId": "a753f57f-f0ec-4bb5-e971-1a0afad9424f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using sample list: keystroke_sample_files_blend.txt  |  files: 1,000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CFG.setdefault(\"keystroke\", {})\n",
        "CFG[\"keystroke\"].setdefault(\"window_sec\", 10)\n",
        "CFG[\"keystroke\"].setdefault(\"stride_sec\", 5)\n",
        "CFG[\"keystroke\"].setdefault(\"min_events_per_window\", 8)\n",
        "print(f\"Window params → window={CFG['keystroke']['window_sec']}s, \"\n",
        "      f\"stride={CFG['keystroke']['stride_sec']}s, \"\n",
        "      f\"min_events={CFG['keystroke']['min_events_per_window']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeXrIlmvBCrF",
        "outputId": "01ea2f4a-9479-4e54-da26-6b17bd907972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window params → window=30s, stride=15s, min_events=20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Keystroke windowization (Aalto .txt, concat per user, lenient windows)\n",
        "# =========================\n",
        "# --- Helper functions (only define if missing in your notebook) ---\n",
        "def _defined(name: str) -> bool:\n",
        "    return name in globals() and callable(globals()[name])\n",
        "\n",
        "if not _defined('parse_keystroke_file_aalto_txt'):\n",
        "    def parse_keystroke_file_aalto_txt(path: str) -> pl.DataFrame | None:\n",
        "        \"\"\"Parse Aalto .txt (tab-separated) -> ['user','session','press_time','dwell'] in seconds.\"\"\"\n",
        "        try:\n",
        "            df = pl.read_csv(\n",
        "                path, separator='\\t', has_header=True,\n",
        "                infer_schema_length=10000, truncate_ragged_lines=True,\n",
        "                ignore_errors=True, low_memory=True,\n",
        "            )\n",
        "        except Exception:\n",
        "            return None\n",
        "        df = df.rename({c: c.lower().strip() for c in df.columns})\n",
        "        req = ['participant_id','test_section_id','press_time','release_time']\n",
        "        if any(c not in df.columns for c in req):\n",
        "            return None\n",
        "        df = df.with_columns([\n",
        "            pl.col('participant_id').cast(pl.Utf8).alias('user'),\n",
        "            pl.col('test_section_id').cast(pl.Utf8).alias('session'),\n",
        "            pl.col('press_time').cast(pl.Float64).alias('press_ms'),\n",
        "            pl.col('release_time').cast(pl.Float64).alias('release_ms'),\n",
        "        ]).with_columns([\n",
        "            (pl.col('press_ms')/1000.0).alias('press_time'),\n",
        "            ((pl.col('release_ms')-pl.col('press_ms'))/1000.0).alias('dwell'),\n",
        "        ])\n",
        "        out = df.select(['user','session','press_time','dwell']).filter(\n",
        "            pl.col('press_time').is_not_null() &\n",
        "            pl.col('dwell').is_not_null() &\n",
        "            (pl.col('dwell') >= 0.0) & (pl.col('dwell') <= 5.0)\n",
        "        )\n",
        "        return out if not out.is_empty() else None\n",
        "\n",
        "if not _defined('concat_per_user_keystrokes'):\n",
        "    def concat_per_user_keystrokes(frames: list[pl.DataFrame]) -> pd.DataFrame:\n",
        "        \"\"\"Concat sessions per user with time offsets; output pandas DF ['user','t','dwell'].\"\"\"\n",
        "        if not frames:\n",
        "            return pd.DataFrame(columns=['user','t','dwell'])\n",
        "        df = pl.concat(frames, how='vertical').select(['user','session','press_time','dwell'])\n",
        "        pdf = df.to_pandas()\n",
        "        users = pdf['user'].unique().tolist()\n",
        "        big_rows = []\n",
        "        for u in users:\n",
        "            g = pdf[pdf['user']==u].copy()\n",
        "            g.sort_values(['session','press_time'], inplace=True)\n",
        "            t_offset = 0.0\n",
        "            for sid, seg in g.groupby('session'):\n",
        "                seg = seg.sort_values('press_time')\n",
        "                if len(seg)==0:\n",
        "                    continue\n",
        "                t = seg['press_time'].to_numpy(dtype=np.float64) + t_offset\n",
        "                dwell = seg['dwell'].to_numpy(dtype=np.float32)\n",
        "                big_rows.append(pd.DataFrame({'user': u, 't': t, 'dwell': dwell}))\n",
        "                t_offset = float(t[-1] + 1.0)  # 1s gap between sessions\n",
        "        big = pd.concat(big_rows, ignore_index=True) if big_rows else pd.DataFrame(columns=['user','t','dwell'])\n",
        "        big.dropna(subset=['t','dwell'], inplace=True)\n",
        "        return big\n",
        "\n",
        "if not _defined('make_keystroke_windows_concat'):\n",
        "    def make_keystroke_windows_concat(pdf: pd.DataFrame, window_sec: int, stride_sec: int, min_events: int):\n",
        "        \"\"\"Slide windows over per-user timelines; return list of (user,'concat',t_start,t_end, seq[N,2]).\"\"\"\n",
        "        out = []\n",
        "        for u, g in pdf.groupby('user'):\n",
        "            g = g.sort_values('t')\n",
        "            t = g['t'].to_numpy(dtype=np.float32)\n",
        "            dw = g['dwell'].to_numpy(dtype=np.float32)\n",
        "            if t.size < 3:\n",
        "                continue\n",
        "            t0, tmax = float(t.min()), float(t.max())\n",
        "            start = t0\n",
        "            while start + window_sec <= tmax + 1e-6:\n",
        "                m = (t >= start) & (t < start + window_sec)\n",
        "                if int(m.sum()) >= min_events:\n",
        "                    t_sub = t[m]\n",
        "                    dw_sub = dw[m]\n",
        "                    ikg = np.diff(t_sub, prepend=t_sub[0])  # inter-key gap\n",
        "                    seq = np.stack([dw_sub, ikg], axis=1).astype(np.float32)\n",
        "                    out.append((u, 'concat', float(start), float(start+window_sec), seq))\n",
        "                start += stride_sec\n",
        "        return out\n",
        "\n",
        "  # --- Parse ONLY the sampled files, with light progress prints ---\n",
        "parsed = []\n",
        "t0 = time.time()\n",
        "for i, fp in enumerate(aalto_files, 1):\n",
        "    dfp = parse_keystroke_file_aalto_txt(fp)\n",
        "    if dfp is not None and not dfp.is_empty():\n",
        "        parsed.append(dfp)\n",
        "    if i % 200 == 0:\n",
        "        took = time.time() - t0\n",
        "        print(f\"Parsed {i}/{len(aalto_files)} files  |  parsed_ok={len(parsed)}  |  {took:.1f}s elapsed\")\n",
        "\n",
        "if not parsed:\n",
        "    raise SystemExit(\"Parsed 0 usable keystroke files from the sample list — check paths/schema.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx05-ehAvwBs",
        "outputId": "63c8960f-4cbd-424b-a440-4ed83cf28a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsed 200/1000 files  |  parsed_ok=200  |  200.7s elapsed\n",
            "Parsed 400/1000 files  |  parsed_ok=400  |  271.7s elapsed\n",
            "Parsed 600/1000 files  |  parsed_ok=600  |  342.7s elapsed\n",
            "Parsed 800/1000 files  |  parsed_ok=800  |  412.9s elapsed\n",
            "Parsed 1000/1000 files  |  parsed_ok=1000  |  488.0s elapsed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import os, pandas as pd\n",
        "\n",
        "AALTO_TXT_DIR = RAW/'aalto'/'Keystrokes'/'files'\n",
        "all_files = sorted([str(p) for p in Path(AALTO_TXT_DIR).glob('*.txt')])\n",
        "\n",
        "# Get file sizes (metadata-only, fast)\n",
        "sizes = []\n",
        "for fp in all_files:\n",
        "    try:\n",
        "        sizes.append(os.stat(fp).st_size)\n",
        "    except FileNotFoundError:\n",
        "        sizes.append(0)\n",
        "\n",
        "df = pd.DataFrame({'file': all_files, 'bytes': sizes})\n",
        "df = df.sort_values('bytes', ascending=False).reset_index(drop=True)\n",
        "\n",
        "TOPK = 10_000\n",
        "top_df = df.head(min(TOPK, len(df)))\n",
        "OUT_LIST = INTER/'keystroke_sample_files_top10k.txt'\n",
        "top_df['file'].to_csv(OUT_LIST, index=False, header=False)\n",
        "print(\"Saved:\", OUT_LIST, \"count:\", len(top_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "livPm1Wu_zar",
        "outputId": "96a8e2ef-71be-4259-c489-4537aa17a087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/mindease_behavior/intermediate/keystroke_sample_files_top10k.txt count: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# FAST BATCH KEYSTROKE WINDOWIZATION (10k files)\n",
        "# - Uses your precomputed lists (by_size or blend)\n",
        "# - Copies by tar streaming (fewer Drive roundtrips)\n",
        "# - Polars scans many files directly (no merge)\n",
        "# - Original window config: 30s / 15s / min 20 events\n",
        "# ============================================\n",
        "import os, time, shlex, subprocess, gc\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import polars as pl\n",
        "\n",
        "# ---------- Paths ----------\n",
        "DRIVE_ROOT = Path('/content/drive/MyDrive/mindease_behavior')\n",
        "RAW   = DRIVE_ROOT/'raw'\n",
        "INTER = DRIVE_ROOT/'intermediate'\n",
        "ART   = DRIVE_ROOT/'artifacts'\n",
        "for p in [RAW, INTER, ART]: p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "AALTO_DIR = RAW/'aalto'\n",
        "LOCAL_ROOT = Path('/content/aalto_batches')   # local SSD workspace\n",
        "LOCAL_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------- Choose which saved list to use ----------\n",
        "BLEND_LIST = INTER/'keystroke_sample_files_blend.txt'\n",
        "SIZE_LIST  = INTER/'keystroke_sample_files_by_size.txt'\n",
        "\n",
        "# Prefer by_size for throughput (bigger files first). Switch to BLEND for more diversity.\n",
        "SAMPLE_LIST = INTER/'keystroke_sample_files_top10k.txt'\n",
        "assert SAMPLE_LIST.exists(), \"No sample list found in /intermediate (by_size or blend).\"\n",
        "\n",
        "# Load *all* file paths in the chosen list\n",
        "all_list_files = [ln.strip() for ln in open(SAMPLE_LIST) if ln.strip()]\n",
        "print(f\"Sample list: {SAMPLE_LIST.name}  |  total paths in list: {len(all_list_files):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObJfn3Ee2_Tl",
        "outputId": "c0a3ed65-7377-4e26-8b00-1522ae3bf366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample list: keystroke_sample_files_top10k.txt  |  total paths in list: 10,000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Window config (original) ----------\n",
        "CFG = globals().get('CFG', {})\n",
        "CFG.setdefault('keystroke', {})\n",
        "CFG['keystroke'].update({\n",
        "    \"window_sec\": 30,\n",
        "    \"stride_sec\": 15,\n",
        "    \"min_events_per_window\": 20,\n",
        "})\n",
        "print(\"Window params:\", CFG['keystroke'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMlpTTNpmwiA",
        "outputId": "e2b8f15c-89e9-4a36-e188-79befd27fac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window params: {'window_sec': 30, 'stride_sec': 15, 'max_users': 30, 'max_sessions_per_user': 6, 'min_events_per_window': 20}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Window config (original) ----------\n",
        "CFG = globals().get('CFG', {})\n",
        "CFG.setdefault('keystroke', {})\n",
        "CFG['keystroke'].update({\n",
        "    \"window_sec\": 30,\n",
        "    \"stride_sec\": 15,\n",
        "    \"min_events_per_window\": 20,\n",
        "})\n",
        "print(\"Window params:\", CFG['keystroke'])\n",
        "\n",
        "# ---------- Scale controls ----------\n",
        "TOTAL_FILES = 2000      # target number of files to process from the list\n",
        "BATCH_SIZE  = 1_000       # local memory friendly; try 2000 if you have headroom\n",
        "\n",
        "# Cap to what's available in the list\n",
        "TOTAL_FILES = min(TOTAL_FILES, len(all_list_files))\n",
        "files_to_use = all_list_files[:TOTAL_FILES]\n",
        "num_batches = (len(files_to_use) + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "print(f\"Planning {num_batches} batch(es): {len(files_to_use):,} files, batch size {BATCH_SIZE}\")\n",
        "\n",
        "# ---------- Utils ----------\n",
        "def read_header_cols(path_str: str) -> list[str]:\n",
        "    # Read just the header row quickly; tolerant to encoding\n",
        "    with open(path_str, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        line = f.readline().rstrip('\\n\\r')\n",
        "    # split on tab\n",
        "    return line.split('\\t') if line else []\n",
        "\n",
        "def normalize(col: str) -> str:\n",
        "    return col.strip().lower()\n",
        "\n",
        "def process_batch(batch_idx: int, batch_paths: list[str]):\n",
        "    batch_dir = LOCAL_ROOT / f\"batch_{batch_idx:03d}\"\n",
        "    out_npz   = INTER / f\"keystroke_windows_batch{batch_idx:03d}.npz\"\n",
        "    if out_npz.exists():\n",
        "        print(f\"[batch {batch_idx}] already exists -> skipping\")\n",
        "        return\n",
        "\n",
        "    t0 = time.time()\n",
        "    batch_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # 1) Copy with tar stream (Drive -> local)\n",
        "    filelist_txt = batch_dir / \"files.txt\"\n",
        "    with open(filelist_txt, \"w\") as f:\n",
        "        for p in batch_paths:\n",
        "            f.write(p + \"\\n\")\n",
        "\n",
        "    print(f\"[batch {batch_idx}] copying {len(batch_paths)} files via tar stream ...\")\n",
        "    subprocess.run(\n",
        "        [\"bash\",\"-lc\",\n",
        "         f\"tar -cf - -T {shlex.quote(str(filelist_txt))} | (cd {shlex.quote(str(batch_dir))} && tar -xf -)\"],\n",
        "        check=True\n",
        "    )\n",
        "    print(f\"[batch {batch_idx}] copy done in {time.time()-t0:.1f}s\")\n",
        "\n",
        "    # 2) Build list of local paths in same order; ensure they exist\n",
        "    local_paths = []\n",
        "    for p in batch_paths:\n",
        "        lp = batch_dir.joinpath(p.lstrip('/'))\n",
        "        if lp.exists():\n",
        "            local_paths.append(str(lp))\n",
        "    if not local_paths:\n",
        "        print(f\"[batch {batch_idx}] nothing copied; skipping\")\n",
        "        return\n",
        "\n",
        "    # 3) PER-BATCH HEADER SNIFF across all local files\n",
        "    # Gather all header variants present in this batch\n",
        "    observed_headers = set()\n",
        "    # Limit header sniff to e.g. first 500 files for speed (adjust if needed)\n",
        "    SAMPLE_H = min(len(local_paths), 500)\n",
        "    for path in local_paths[:SAMPLE_H]:\n",
        "        for col in read_header_cols(path):\n",
        "            observed_headers.add(col)\n",
        "\n",
        "    # Build lowercase map for all observed headers (each original -> normalized lower)\n",
        "    lower_map = {orig: normalize(orig) for orig in observed_headers}\n",
        "\n",
        "    # Build robust schema_overrides covering ALL observed variants\n",
        "    # Anything that normalizes to these keys gets forced to the desired dtype\n",
        "    time_keys = {\"press_time\", \"release_time\"}\n",
        "    id_keys   = {\"participant_id\", \"test_section_id\"}\n",
        "\n",
        "    schema_overrides = {}\n",
        "    for orig, low in lower_map.items():\n",
        "        if low in time_keys:\n",
        "            schema_overrides[orig] = pl.Float64\n",
        "        elif low in id_keys:\n",
        "            schema_overrides[orig] = pl.Utf8\n",
        "        # others: let Polars infer lazily\n",
        "\n",
        "    # 4) Polars lazy scan of many files with robust schema + rename\n",
        "    # Note: schema_overrides expects ORIGINAL column names (per file); then we rename to lowercase\n",
        "    try:\n",
        "        lazy = pl.scan_csv(\n",
        "            local_paths,\n",
        "            separator='\\t',\n",
        "            has_header=True,\n",
        "            infer_schema_length=0,\n",
        "            schema_overrides=schema_overrides,  # robust across header variants\n",
        "            ignore_errors=True,\n",
        "            null_values=[\"\", \"NA\", \"NaN\"],\n",
        "            low_memory=True,\n",
        "        )\n",
        "    except TypeError:\n",
        "        # older polars uses 'dtypes' instead of 'schema_overrides'\n",
        "        lazy = pl.scan_csv(\n",
        "            local_paths,\n",
        "            separator='\\t',\n",
        "            has_header=True,\n",
        "            infer_schema_length=0,\n",
        "            dtypes=schema_overrides,\n",
        "            ignore_errors=True,\n",
        "            null_values=[\"\", \"NA\", \"NaN\"],\n",
        "            low_memory=True,\n",
        "        )\n",
        "\n",
        "    # Normalize headers to lowercase (whatever they were)\n",
        "    # We can only rename keys that actually exist; use intersection\n",
        "    # Collect the current columns from the lazy scan plan\n",
        "    # (Polars will union schemas; we can just attempt a broad rename)\n",
        "    rename_map = {}\n",
        "    # Try to obtain columns from a small eager read of just header rows of first file\n",
        "    # but safer: use the observed headers set (covers batch)\n",
        "    for orig in observed_headers:\n",
        "        rename_map[orig] = lower_map[orig]\n",
        "    lazy = lazy.rename(rename_map)\n",
        "\n",
        "    # Required columns after rename\n",
        "    req = ['participant_id','test_section_id','press_time','release_time']\n",
        "    # It's possible some files miss a column; filtering after building features will drop them\n",
        "    missing_after_rename = [c for c in req if c not in lazy.columns]\n",
        "    if missing_after_rename:\n",
        "        # Try to continue — rows missing required cols will be filtered out by .filter below\n",
        "        print(f\"[batch {batch_idx}] Warning: some required cols absent in union schema: {missing_after_rename}\")\n",
        "\n",
        "    # 5) Build features & dynamic windows\n",
        "    lazy = (\n",
        "        lazy\n",
        "        .with_columns([\n",
        "            pl.col('participant_id').cast(pl.Utf8).alias('user'),\n",
        "            pl.col('test_section_id').cast(pl.Utf8).alias('session'),\n",
        "            (pl.col('press_time').cast(pl.Float64) / 1000.0).alias('press_s'),\n",
        "            ((pl.col('release_time').cast(pl.Float64) - pl.col('press_time').cast(pl.Float64)) / 1000.0).alias('dwell_s'),\n",
        "        ])\n",
        "        .filter(\n",
        "            pl.col('press_s').is_not_null() &\n",
        "            pl.col('dwell_s').is_not_null() &\n",
        "            (pl.col('dwell_s') >= 0.0) & (pl.col('dwell_s') <= 5.0)\n",
        "        )\n",
        "        .with_columns(\n",
        "            (pl.datetime(1970,1,1) + pl.duration(seconds=pl.col('press_s'))).alias('press_dt')\n",
        "        )\n",
        "        .select(['user','session','press_s','dwell_s','press_dt'])\n",
        "        .sort(['user','session','press_dt'])\n",
        "    )\n",
        "\n",
        "    every  = f\"{CFG['keystroke']['stride_sec']}s\"\n",
        "    period = f\"{CFG['keystroke']['window_sec']}s\"\n",
        "\n",
        "    grouped = (\n",
        "        lazy\n",
        "        .group_by_dynamic(\n",
        "            index_column='press_dt',\n",
        "            every=every,\n",
        "            period=period,\n",
        "            group_by=['user','session'],\n",
        "            closed='left',\n",
        "            label='left',\n",
        "            start_by='datapoint'\n",
        "        )\n",
        "        .agg([\n",
        "            pl.min('press_s').alias('t_start'),\n",
        "            pl.max('press_s').alias('t_end'),\n",
        "            pl.col('press_s').implode().alias('pt_list'),\n",
        "            pl.col('dwell_s').implode().alias('dwell_list'),\n",
        "            pl.len().alias('n_events'),\n",
        "        ])\n",
        "        .filter(pl.col('n_events') >= CFG['keystroke']['min_events_per_window'])\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        win_df = grouped.collect(streaming=True)\n",
        "    except Exception:\n",
        "        win_df = grouped.collect()\n",
        "    print(f\"[batch {batch_idx}] windows table rows:\", win_df.shape[0])\n",
        "\n",
        "    # 6) Pack sequences & save batch npz\n",
        "    def build_seq_row(pt_list, dwell_list):\n",
        "        pt = np.asarray(pt_list, dtype=np.float64).reshape(-1)\n",
        "        dw = np.asarray(dwell_list, dtype=np.float64).reshape(-1)\n",
        "        n = min(pt.size, dw.size)\n",
        "        if n == 0:\n",
        "            return None\n",
        "        pt, dw = pt[:n], dw[:n]\n",
        "        m = np.isfinite(pt) & np.isfinite(dw)\n",
        "        if not np.any(m):\n",
        "            return None\n",
        "        pt, dw = pt[m], dw[m]\n",
        "        ikg = np.diff(pt, prepend=pt[:1])\n",
        "        return np.column_stack((dw.astype(np.float32), ikg.astype(np.float32)))\n",
        "\n",
        "    ks_windows = []\n",
        "    for row in win_df.iter_rows(named=True):\n",
        "        seq = build_seq_row(row['pt_list'], row['dwell_list'])\n",
        "        if seq is None or seq.shape[0] < CFG['keystroke']['min_events_per_window']:\n",
        "            continue\n",
        "        ks_windows.append((row['user'], row['session'], float(row['t_start']), float(row['t_end']), seq))\n",
        "\n",
        "    np.savez_compressed(out_npz, windows=np.array(ks_windows, dtype=object), allow_pickle=True)\n",
        "    print(f\"[batch {batch_idx}] saved: {out_npz}  windows: {len(ks_windows)}\")\n",
        "\n",
        "    # 7) Cleanup local to free space\n",
        "    try:\n",
        "        for p in batch_dir.glob(\"**/*\"):\n",
        "            if p.is_file(): p.unlink()\n",
        "        for p in sorted(batch_dir.glob(\"**/*\"), reverse=True):\n",
        "            if p.is_dir(): p.rmdir()\n",
        "        batch_dir.rmdir()\n",
        "    except Exception:\n",
        "        pass\n",
        "    gc.collect()\n",
        "\n",
        "# ---------- Run batches ----------\n",
        "for b in range(num_batches):\n",
        "    lo, hi = b*BATCH_SIZE, min((b+1)*BATCH_SIZE, len(files_to_use))\n",
        "    batch_paths = files_to_use[lo:hi]\n",
        "    process_batch(b, batch_paths)\n",
        "\n",
        "# ---------- Combine all batch npz into one ----------\n",
        "all_npz = sorted(INTER.glob(\"keystroke_windows_batch*.npz\"))\n",
        "print(\"Combining\", len(all_npz), \"batch files ...\")\n",
        "all_windows = []\n",
        "for npz in all_npz:\n",
        "    arr = np.load(npz, allow_pickle=True)['windows']\n",
        "    if len(arr) > 0:\n",
        "        all_windows.extend(arr.tolist())\n",
        "\n",
        "KS_WIN_NPZ = INTER / \"keystroke_windows.npz\"\n",
        "np.savez_compressed(KS_WIN_NPZ, windows=np.array(all_windows, dtype=object), allow_pickle=True)\n",
        "print(\"Final keystroke windows:\", len(all_windows))\n",
        "print(\"Saved combined:\", KS_WIN_NPZ)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RA5AD3wODI8",
        "outputId": "edf08fef-91f2-48e1-d3f9-95ae1c283e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Window params: {'window_sec': 30, 'stride_sec': 15, 'max_users': 30, 'max_sessions_per_user': 6, 'min_events_per_window': 20}\n",
            "Planning 2 batch(es): 2,000 files, batch size 1000\n",
            "[batch 0] copying 1000 files via tar stream ...\n",
            "[batch 0] copy done in 409.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2575512414.py:131: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
            "  missing_after_rename = [c for c in req if c not in lazy.columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[batch 0] windows table rows: 20150\n",
            "[batch 0] saved: /content/drive/MyDrive/mindease_behavior/intermediate/keystroke_windows_batch000.npz  windows: 20150\n",
            "[batch 1] copying 1000 files via tar stream ...\n",
            "[batch 1] copy done in 227.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2575512414.py:131: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
            "  missing_after_rename = [c for c in req if c not in lazy.columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[batch 1] windows table rows: 18003\n",
            "[batch 1] saved: /content/drive/MyDrive/mindease_behavior/intermediate/keystroke_windows_batch001.npz  windows: 18003\n",
            "Combining 10 batch files ...\n",
            "Final keystroke windows: 172053\n",
            "Saved combined: /content/drive/MyDrive/mindease_behavior/intermediate/keystroke_windows.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-clone Balabit with git-lfs so data files are real\n",
        "!apt-get -y -qq install git-lfs\n",
        "!git lfs install\n",
        "!rm -rf \"{BALABIT_DIR}\"\n",
        "!git clone https://github.com/balabit/Mouse-Dynamics-Challenge \"{BALABIT_DIR}\"\n",
        "\n",
        "# Quick sanity: count ALL regular files (no extension filter)\n",
        "import os\n",
        "cnt = 0\n",
        "for root, dirs, files in os.walk(BALABIT_DIR):\n",
        "    for f in files:\n",
        "        p = os.path.join(root, f)\n",
        "        if os.path.isfile(p):\n",
        "            cnt += 1\n",
        "print(\"Balabit regular files:\", cnt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqS8cYYs25fP",
        "outputId": "e3348c26-d579-400b-b47a-0955e32f0d45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into '/content/drive/MyDrive/mindease_behavior/raw/balabit'...\n",
            "remote: Enumerating objects: 1711, done.\u001b[K\n",
            "remote: Total 1711 (delta 0), reused 0 (delta 0), pack-reused 1711 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1711/1711), 42.60 MiB | 11.37 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "Updating files: 100% (1678/1678), done.\n",
            "Balabit regular files: 1704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make windows easier to get\n",
        "CFG['mouse'].update({\n",
        "    \"window_sec\": 6,           # small to tolerate short sessions\n",
        "    \"stride_sec\": 3,\n",
        "    \"min_events_per_window\": 6\n",
        "})"
      ],
      "metadata": {
        "id": "69qQHAM0m0qG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Mouse windowization (no-extension files, robust reader + concat per user)\n",
        "# =========================\n",
        "import os, csv, glob, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Make windows easier to get\n",
        "CFG['mouse'].update({\n",
        "    \"window_sec\": 6,           # small to tolerate short sessions\n",
        "    \"stride_sec\": 3,\n",
        "    \"min_events_per_window\": 6\n",
        "})\n",
        "\n",
        "BALABIT_ROOT = BALABIT_DIR\n",
        "\n",
        "def list_all_files(root: Path):\n",
        "    files = []\n",
        "    for r, d, fs in os.walk(root):\n",
        "        for f in fs:\n",
        "            p = os.path.join(r, f)\n",
        "            if os.path.isfile(p):\n",
        "                files.append(p)\n",
        "    # keep only those under training_files/ or test_files/\n",
        "    files = [p for p in files if ('training_files' in p or 'test_files' in p)]\n",
        "    files = sorted(list(dict.fromkeys(files)))\n",
        "    return files\n",
        "\n",
        "def sniff_sep(path, default=','):\n",
        "    try:\n",
        "        with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            sample = ''.join([next(f) for _ in range(50)])\n",
        "        return csv.Sniffer().sniff(sample, delimiters=',\\t;| ').delimiter\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "def read_balabit_file_any(path):\n",
        "    \"\"\"\n",
        "    Return DataFrame with columns t(sec), x, y. Tolerates header/no-header and any delimiter.\n",
        "    Known Balabit columns: record_ts, client_ts, button, state, x, y\n",
        "    \"\"\"\n",
        "    # try with sniffed sep and header\n",
        "    sep = sniff_sep(path, default=',')\n",
        "    try:\n",
        "        df = pd.read_csv(path, sep=sep, engine='python', on_bad_lines='skip')\n",
        "    except Exception:\n",
        "        # try tab\n",
        "        try:\n",
        "            df = pd.read_csv(path, sep='\\t', engine='python', on_bad_lines='skip')\n",
        "        except Exception:\n",
        "            # last resort: whitespace\n",
        "            try:\n",
        "                df = pd.read_csv(path, delim_whitespace=True, engine='python', on_bad_lines='skip', header=None)\n",
        "            except Exception:\n",
        "                return None\n",
        "\n",
        "    cols = [str(c).strip().lower() for c in df.columns]\n",
        "    df.columns = cols\n",
        "\n",
        "    # choose time col\n",
        "    tcol = None\n",
        "    for c in ('record_ts','client_ts','time','timestamp','t'):\n",
        "        if c in cols:\n",
        "            tcol = c; break\n",
        "    if tcol is None:\n",
        "        tcol = cols[0]  # guess first col\n",
        "\n",
        "    # choose x,y\n",
        "    xcol = 'x' if 'x' in cols else (cols[-2] if len(cols)>=2 else None)\n",
        "    ycol = 'y' if 'y' in cols else (cols[-1] if len(cols)>=1 else None)\n",
        "    if xcol is None or ycol is None or xcol==tcol or ycol==tcol:\n",
        "        return None\n",
        "\n",
        "    t = pd.to_numeric(df[tcol], errors='coerce').astype(float)\n",
        "    x = pd.to_numeric(df[xcol], errors='coerce').astype(float)\n",
        "    y = pd.to_numeric(df[ycol], errors='coerce').astype(float)\n",
        "    keep = (~t.isna()) & (~x.isna()) & (~y.isna())\n",
        "    t, x, y = t[keep].values, x[keep].values, y[keep].values\n",
        "    if t.size < 3:\n",
        "        return None\n",
        "\n",
        "    # normalize time units → seconds\n",
        "    dt = np.diff(t, prepend=t[0])\n",
        "    pos = dt[dt > 0]\n",
        "    med_dt = np.median(pos) if pos.size else 0.0\n",
        "    if med_dt > 10:       # looks like ms\n",
        "        t = t / 1000.0\n",
        "    elif med_dt < 1e-3:   # looks like microseconds\n",
        "        t = t / 1e6\n",
        "\n",
        "    # shift to zero\n",
        "    t = t - t.min()\n",
        "\n",
        "    return pd.DataFrame({'t': t, 'x': x, 'y': y})\n",
        "\n",
        "def user_from_path(p: str):\n",
        "    # training_files/<USER>/... or test_files/<USER>/...\n",
        "    parts = Path(p).parts\n",
        "    # find index of training_files or test_files\n",
        "    for i, seg in enumerate(parts):\n",
        "        if seg in ('training_files', 'test_files'):\n",
        "            if i+1 < len(parts):\n",
        "                return parts[i+1]\n",
        "    return 'unknown'\n",
        "\n",
        "def seq_from_df(df: pd.DataFrame):\n",
        "    df = df.sort_values('t')\n",
        "    t = df['t'].to_numpy(dtype=np.float32)\n",
        "    x = df['x'].to_numpy(dtype=np.float32)\n",
        "    y = df['y'].to_numpy(dtype=np.float32)\n",
        "    if len(t) < 3:\n",
        "        return None, None\n",
        "    dx = np.diff(x, prepend=x[0])\n",
        "    dy = np.diff(y, prepend=y[0])\n",
        "    dt = np.clip(np.diff(t, prepend=t[0]), 1e-3, None)\n",
        "    speed = np.sqrt(dx**2 + dy**2) / dt\n",
        "    accel = np.diff(speed, prepend=speed[0]) / dt\n",
        "    jerk  = np.diff(accel, prepend=accel[0]) / dt\n",
        "    seq = np.stack([dx, dy, dt, speed, accel, jerk], axis=1).astype(np.float32)\n",
        "    return t, seq\n",
        "\n",
        "def make_mouse_windows_concat(root, window_sec, stride_sec, min_events):\n",
        "    files = list_all_files(Path(root))\n",
        "    print(\"Files considered:\", len(files))\n",
        "    # read all files into per-user buckets\n",
        "    per_user = {}\n",
        "    for fp in files:\n",
        "        df = read_balabit_file_any(fp)\n",
        "        if df is None or len(df) < 3:\n",
        "            continue\n",
        "        uid = user_from_path(fp)\n",
        "        per_user.setdefault(uid, []).append(df)\n",
        "\n",
        "    rows = []\n",
        "    for uid, dfs in per_user.items():\n",
        "        if not dfs:\n",
        "            continue\n",
        "        # concatenate with offsets so they form a continuous stream\n",
        "        t_offset = 0.0\n",
        "        big = []\n",
        "        for df in sorted(dfs, key=lambda d: d['t'].min()):\n",
        "            dfa = df.copy()\n",
        "            dfa['t'] = dfa['t'] + t_offset\n",
        "            big.append(dfa)\n",
        "            t_offset = float(dfa['t'].max() + 1.0)\n",
        "        bigdf = pd.concat(big, ignore_index=True)\n",
        "\n",
        "        t, seq = seq_from_df(bigdf)\n",
        "        if t is None:\n",
        "            continue\n",
        "        t0, tmax = float(t.min()), float(t.max())\n",
        "        start = t0\n",
        "        while start + window_sec <= tmax + 1e-6:\n",
        "            m = (t >= start) & (t < start + window_sec)\n",
        "            if int(m.sum()) >= min_events:\n",
        "                rows.append((uid, float(start), float(start + window_sec), seq[m]))\n",
        "            start += stride_sec\n",
        "    print(f\"[Balabit concat] users={len(per_user)} windows={len(rows)}\")\n",
        "    return rows\n",
        "\n",
        "mouse_windows = make_mouse_windows_concat(\n",
        "    BALABIT_ROOT,\n",
        "    CFG['mouse']['window_sec'],\n",
        "    CFG['mouse']['stride_sec'],\n",
        "    CFG['mouse']['min_events_per_window']\n",
        ")\n",
        "\n",
        "# Save (object array for variable-length seqs)\n",
        "MOUSE_WIN_NPZ = INTER / 'mouse_windows.npz'\n",
        "np.savez_compressed(MOUSE_WIN_NPZ, windows=np.array(mouse_windows, dtype=object), allow_pickle=True)\n",
        "print(\"Mouse windows:\", len(mouse_windows))\n",
        "print(\"Saved:\", MOUSE_WIN_NPZ)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzMYU7Wn1AI1",
        "outputId": "a02fe0c5-a3b8-42e9-85c7-6d61e723f07c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files considered: 1676\n",
            "[Balabit concat] users=10 windows=215980\n",
            "Mouse windows: 215980\n",
            "Saved: /content/drive/MyDrive/mindease_behavior/intermediate/mouse_windows.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 7) Robust NPZ inspector + loader (fixes \"0 sequences\" issue)\n",
        "import os, glob, pprint, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "DRIVE_ROOT = Path('/content/drive/MyDrive/mindease_behavior')\n",
        "INTER = DRIVE_ROOT/'intermediate'\n",
        "ART  = DRIVE_ROOT/'artifacts'\n",
        "ART.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "KS_NPZ = INTER/'keystroke_windows.npz'\n",
        "MOUSE_NPZ = INTER/'mouse_windows.npz'\n",
        "\n",
        "def describe_file(p: Path, label: str):\n",
        "    print(f\"{label}: {p}  |  exists={p.exists()}  |  sizeMB={p.stat().st_size/1e6:.2f}\" if p.exists() else f\"{label}: MISSING -> {p}\")\n",
        "describe_file(KS_NPZ, \"Keystroke\")\n",
        "describe_file(MOUSE_NPZ, \"Mouse\")\n",
        "\n",
        "def inspect_npz(npz_path: Path, prefer_key='windows', peek=3):\n",
        "    if not npz_path.exists():\n",
        "        print(\"  → file missing\")\n",
        "        return None, None\n",
        "    data = np.load(npz_path, allow_pickle=True)\n",
        "    print(\"Keys:\", list(data.files))\n",
        "    key = prefer_key if prefer_key in data.files else data.files[0]\n",
        "    arr = data[key]\n",
        "    print(f\"Chosen key: {key} | dtype={arr.dtype} | ndim={arr.ndim} | shape={getattr(arr,'shape',None)} | size={arr.size}\")\n",
        "    # Print a few examples safely\n",
        "    def safe_repr(x):\n",
        "        r = repr(x)\n",
        "        return r[:200] + (\"...\" if len(r)>200 else \"\")\n",
        "    try:\n",
        "        if arr.ndim == 0:\n",
        "            obj = arr.item()\n",
        "            print(\"0-D object; type(obj):\", type(obj))\n",
        "            if isinstance(obj, (list, tuple)):\n",
        "                print(\"First items:\", [safe_repr(obj[i]) for i in range(min(peek, len(obj)))])\n",
        "            else:\n",
        "                print(\"Scalar object:\", safe_repr(obj))\n",
        "        else:\n",
        "            n = min(peek, len(arr))\n",
        "            print(f\"First {n} items:\")\n",
        "            for i in range(n):\n",
        "                print(\" \", i, \"→\", safe_repr(arr[i]))\n",
        "    except Exception as e:\n",
        "        print(\"Peek error:\", e)\n",
        "    return data, key\n",
        "\n",
        "print(\"\\n--- Inspect keystroke NPZ ---\")\n",
        "ks_data, ks_key = inspect_npz(KS_NPZ)\n",
        "\n",
        "print(\"\\n--- Inspect mouse NPZ ---\")\n",
        "mouse_data, mouse_key = inspect_npz(MOUSE_NPZ)\n",
        "\n",
        "# --- Load robustly (fixed for 5-field keystrokes AND 4-field mouse rows) ---\n",
        "def extract_sequences_from_array(arr, min_len=4):\n",
        "    \"\"\"\n",
        "    Accepts:\n",
        "      - 2-D object array where each row is:\n",
        "          * ['user','session', t0, t1, seq]  (keystrokes)\n",
        "          * ['user',            t0, t1, seq]  (mouse)\n",
        "      - 0-D object array containing a list/tuple of the above\n",
        "      - raw 2-D sequences\n",
        "    Returns: (seqs, metas) where:\n",
        "      - seqs: list[np.ndarray[T,C]]\n",
        "      - metas: list[(user, session, t0, t1)]\n",
        "    \"\"\"\n",
        "    seqs, metas = [], []\n",
        "\n",
        "    # Normalize container\n",
        "    if isinstance(arr, np.ndarray) and arr.ndim == 0:\n",
        "        items = arr.item()\n",
        "    else:\n",
        "        items = arr\n",
        "\n",
        "    if isinstance(items, np.ndarray) and items.dtype == object:\n",
        "        # turn into Python list-of-rows\n",
        "        try:\n",
        "            items = items.tolist()\n",
        "        except Exception:\n",
        "            items = [items]\n",
        "\n",
        "    if not isinstance(items, (list, tuple)):\n",
        "        items = [items]\n",
        "\n",
        "    for el in items:\n",
        "        # If it's an object-typed row, convert to list for easy indexing\n",
        "        if isinstance(el, np.ndarray) and el.dtype == object:\n",
        "            el = el.tolist()\n",
        "\n",
        "        seq = None\n",
        "        meta = (\"?\", \"?\", 0.0, 0.0)\n",
        "\n",
        "        if isinstance(el, (list, tuple)):\n",
        "            L = len(el)\n",
        "            if L >= 5:\n",
        "                # ['user','session', t0, t1, seq]\n",
        "                seq = np.asarray(el[-1])\n",
        "                try:\n",
        "                    meta = (str(el[0]), str(el[1]), float(el[2]), float(el[3]))\n",
        "                except Exception:\n",
        "                    pass\n",
        "            elif L == 4:\n",
        "                # ['user', t0, t1, seq]  (mouse)\n",
        "                seq = np.asarray(el[-1])\n",
        "                try:\n",
        "                    meta = (str(el[0]), \"?\", float(el[1]), float(el[2]))\n",
        "                except Exception:\n",
        "                    pass\n",
        "            elif L == 1 and isinstance(el[0], np.ndarray) and el[0].ndim == 2:\n",
        "                seq = el[0]\n",
        "            else:\n",
        "                # maybe it's a raw 2-D array wrapped\n",
        "                maybe = np.asarray(el)\n",
        "                if maybe.ndim == 2:\n",
        "                    seq = maybe\n",
        "\n",
        "        elif isinstance(el, np.ndarray) and el.ndim == 2:\n",
        "            seq = el\n",
        "\n",
        "        # keep valid sequences\n",
        "        if seq is not None and seq.ndim == 2 and seq.shape[0] >= min_len:\n",
        "            seqs.append(seq.astype(np.float32))\n",
        "            metas.append(meta)\n",
        "\n",
        "    return seqs, metas\n",
        "\n",
        "def load_sequences(npz_path: Path, prefer_key='windows', min_len=4):\n",
        "    data = np.load(npz_path, allow_pickle=True)\n",
        "    key = prefer_key if prefer_key in data.files else data.files[0]\n",
        "    arr = data[key]\n",
        "    seqs, metas = extract_sequences_from_array(arr, min_len=min_len)\n",
        "    print(f\"Loaded {len(seqs):,} sequences from {npz_path.name} (key='{key}')\")\n",
        "    return seqs, metas\n",
        "\n",
        "print(\"\\n--- Load robustly (fixed typo) ---\")\n",
        "KS_NPZ = Path('/content/drive/MyDrive/mindease_behavior/intermediate/keystroke_windows.npz')\n",
        "MOUSE_NPZ = Path('/content/drive/MyDrive/mindease_behavior/intermediate/mouse_windows.npz')\n",
        "ks_seqs, ks_meta = load_sequences(KS_NPZ, prefer_key='windows', min_len=4)\n",
        "mouse_seqs, mouse_meta = load_sequences(MOUSE_NPZ, prefer_key='windows', min_len=4)\n",
        "print(f\"Keystroke sequences: {len(ks_seqs):,}  example: {ks_seqs[0].shape if ks_seqs else None}\")\n",
        "print(f\"Mouse sequences:     {len(mouse_seqs):,}  example: {mouse_seqs[0].shape if mouse_seqs else None}\")\n",
        "\n",
        "# Recompute channel-wise normalization stats and save\n",
        "def compute_channel_stats(seqs, sample_cap=20000, max_rows=1_000_000):\n",
        "    if not seqs:\n",
        "        return None, None\n",
        "    idx = np.random.choice(len(seqs), size=min(sample_cap, len(seqs)), replace=False)\n",
        "    rows = 0; chunks=[]\n",
        "    for i in idx:\n",
        "        s = seqs[i]\n",
        "        chunks.append(s)\n",
        "        rows += s.shape[0]\n",
        "        if rows >= max_rows:\n",
        "            break\n",
        "    X = np.concatenate(chunks, axis=0)\n",
        "    mu = X.mean(axis=0).astype(np.float32)\n",
        "    sd = (X.std(axis=0) + 1e-8).astype(np.float32)\n",
        "    return mu, sd\n",
        "\n",
        "ks_mean, ks_std = compute_channel_stats(ks_seqs)\n",
        "mouse_mean, mouse_std = compute_channel_stats(mouse_seqs)\n",
        "print(\"KS mean/std:\", ks_mean, ks_std)\n",
        "print(\"Mouse mean/std:\", mouse_mean, mouse_std)\n",
        "\n",
        "ART = Path('/content/drive/MyDrive/mindease_behavior/artifacts')\n",
        "ART.mkdir(parents=True, exist_ok=True)\n",
        "np.savez(ART/'norm_stats.npz', ks_mean=ks_mean, ks_std=ks_std, mouse_mean=mouse_mean, mouse_std=mouse_std)\n",
        "print(\"Saved stats:\", ART/'norm_stats.npz')"
      ],
      "metadata": {
        "id": "BkQ23cYT1CY4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4364138-3253-4102-eba5-243f2f56c3f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keystroke: /content/drive/MyDrive/mindease_behavior/intermediate/keystroke_windows.npz  |  exists=True  |  sizeMB=37.79\n",
            "Mouse: /content/drive/MyDrive/mindease_behavior/intermediate/mouse_windows.npz  |  exists=True  |  sizeMB=73.22\n",
            "\n",
            "--- Inspect keystroke NPZ ---\n",
            "Keys: ['windows', 'allow_pickle']\n",
            "Chosen key: windows | dtype=object | ndim=2 | shape=(172053, 5) | size=860265\n",
            "First 3 items:\n",
            "  0 → array(['100032', '1091453', 1473275637.037, 1473275646.683,\n",
            "       array([[ 0.142     ,  0.        ],\n",
            "              [ 0.104     ,  0.07000017],\n",
            "              [ 0.103     ,  0.04900002],\n",
            "              ...\n",
            "  1 → array(['100032', '1091468', 1473275649.051, 1473275661.7480001,\n",
            "       array([[ 0.104     ,  0.        ],\n",
            "              [ 0.072     ,  0.07999992],\n",
            "              [ 0.079     ,  0.07299995],\n",
            "          ...\n",
            "  2 → array(['100032', '1091487', 1473275664.148, 1473275672.755,\n",
            "       array([[ 0.167     ,  0.        ],\n",
            "              [ 0.08      ,  0.09500003],\n",
            "              [ 0.088     ,  0.16799998],\n",
            "              ...\n",
            "\n",
            "--- Inspect mouse NPZ ---\n",
            "Keys: ['windows', 'allow_pickle']\n",
            "Chosen key: windows | dtype=object | ndim=2 | shape=(215980, 4) | size=863920\n",
            "First 3 items:\n",
            "  0 → array(['user12', 0.0, 6.0,\n",
            "       array([[ 0.00000000e+00,  0.00000000e+00,  1.00000005e-03,\n",
            "                0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
            "              [ 0.00000000e+00,  0.00000...\n",
            "  1 → array(['user12', 6.0, 12.0,\n",
            "       array([[ 1.03400000e+03, -7.51000000e+02,  5.49600029e+00,\n",
            "                2.32523697e+02,  3.97850914e+01,  7.73616943e+02],\n",
            "              [-8.50000000e+01,  2.9000...\n",
            "  2 → array(['user12', 9.0, 15.0,\n",
            "       array([[ 1.03400000e+03, -7.51000000e+02,  5.49600029e+00,\n",
            "                2.32523697e+02,  3.97850914e+01,  7.73616943e+02],\n",
            "              [-8.50000000e+01,  2.9000...\n",
            "\n",
            "--- Load robustly (fixed typo) ---\n",
            "Loaded 172,053 sequences from keystroke_windows.npz (key='windows')\n",
            "Loaded 215,980 sequences from mouse_windows.npz (key='windows')\n",
            "Keystroke sequences: 172,053  example: (64, 2)\n",
            "Mouse sequences:     215,980  example: (30, 6)\n",
            "KS mean/std: [0.11578032 0.25582397] [0.08794915 0.5129864 ]\n",
            "Mouse mean/std: [-4.9335971e-02 -1.5609288e-02  2.8652304e-01  1.7320992e+04\n",
            "  9.4369810e+06  7.3234176e+09] [6.4416071e+02 6.4567743e+02 1.2084090e+01 4.1755028e+05 4.0680973e+08\n",
            " 4.0963375e+11]\n",
            "Saved stats: /content/drive/MyDrive/mindease_behavior/artifacts/norm_stats.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 8) Small temporal encoder + InfoNCE\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TCNEncoder(nn.Module):\n",
        "    def __init__(self, in_ch, emb_dim=64, hidden=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(in_ch, hidden, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(hidden),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(hidden, hidden, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(hidden),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(hidden, emb_dim, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)  # masked mean would be nicer if we had masks\n",
        "\n",
        "    def forward(self, x):   # x: [B, C, T]\n",
        "        z = self.net(x)     # [B, emb, T]\n",
        "        z = self.pool(z).squeeze(-1)  # [B, emb]\n",
        "        z = F.normalize(z, dim=1)\n",
        "        return z\n",
        "\n",
        "\n",
        "class InfoNCELoss(nn.Module):\n",
        "    def __init__(self, temperature=0.2, diag_val=-1e4):\n",
        "        super().__init__()\n",
        "        self.t = float(temperature)\n",
        "        self.diag_val = float(diag_val)  # safe for fp16\n",
        "\n",
        "    def forward(self, z1, z2):\n",
        "        # z1, z2: [B, D], already L2-normalized\n",
        "        B, D = z1.shape\n",
        "\n",
        "        # Concatenate\n",
        "        z = torch.cat([z1, z2], dim=0)  # [2B, D]\n",
        "\n",
        "        # Do similarity math in float32 for stability even under autocast\n",
        "        with amp.autocast(enabled=False):\n",
        "            z32 = z.float()\n",
        "            sim = (z32 @ z32.T) / self.t  # [2B,2B] float32\n",
        "            sim.fill_diagonal_(self.diag_val)  # large negative, fp16-safe\n",
        "\n",
        "            # positives: i<->i+B and i+B<->i\n",
        "            targets = torch.arange(B, device=sim.device)\n",
        "\n",
        "            # Two cross-entropies (float32) – gradients still flow to z1/z2\n",
        "            loss1 = F.cross_entropy(sim[:B, B:], targets)\n",
        "            loss2 = F.cross_entropy(sim[B:, :B], targets)\n",
        "            loss = 0.5 * (loss1 + loss2)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "_Y2lEMm61C6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 9) Training wrapper\n",
        "from torch.cuda import amp\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device:\", device)\n",
        "\n",
        "def train_encoder(\n",
        "    seqs,\n",
        "    mean, std,\n",
        "    in_ch,\n",
        "    out_len,                 # fixed time steps for training (crop/pad)\n",
        "    batch_size=256,\n",
        "    epochs=8,\n",
        "    lr=1e-3,\n",
        "    emb_dim=64,\n",
        "    hidden=128,\n",
        "    aug_strength=1.0,\n",
        "    ckpt_path: Path = None,\n",
        "):\n",
        "    ds = TwoViewDataset(seqs, mean, std, out_len=out_len, aug_strength=aug_strength)\n",
        "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
        "\n",
        "    model = TCNEncoder(in_ch, emb_dim=emb_dim, hidden=hidden).to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    scaler = amp.GradScaler(enabled=(device.type=='cuda'))\n",
        "    criterion = InfoNCELoss(temperature=0.2)\n",
        "\n",
        "    model.train()\n",
        "    best_loss = float('inf')\n",
        "    for ep in range(1, epochs+1):\n",
        "        running = 0.0\n",
        "        for v1, v2 in dl:\n",
        "            v1 = v1.to(device, non_blocking=True).float()\n",
        "            v2 = v2.to(device, non_blocking=True).float()\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with amp.autocast(enabled=(device.type=='cuda')):\n",
        "                z1 = model(v1)\n",
        "                z2 = model(v2)\n",
        "                loss = criterion(z1, z2)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "            running += loss.item() * v1.size(0)\n",
        "        avg = running / (len(dl)*batch_size)\n",
        "        print(f\"Epoch {ep:02d} | loss {avg:.4f}\")\n",
        "        if avg < best_loss:\n",
        "            best_loss = avg\n",
        "            if ckpt_path is not None:\n",
        "                torch.save({'model': model.state_dict(), 'in_ch': in_ch, 'emb_dim': emb_dim, 'hidden': hidden}, ckpt_path)\n",
        "    return model\n",
        "\n",
        "def export_onnx(model, in_ch, out_len, onnx_path: Path):\n",
        "    model.eval()\n",
        "    dummy = torch.zeros(1, in_ch, out_len, device=device)\n",
        "    torch.onnx.export(\n",
        "        model, dummy, str(onnx_path),\n",
        "        input_names=['x'], output_names=['z'],\n",
        "        opset_version=17, do_constant_folding=True,\n",
        "        dynamic_axes={'x': {0: 'batch', 2: 'time'}, 'z': {0: 'batch'}}\n",
        "    )\n",
        "    print(\"Exported:\", onnx_path)"
      ],
      "metadata": {
        "id": "grDww-Jl1Er1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63f7580e-09e6-4cbf-9548-26fe25e7a620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9pEO75vvktg",
        "outputId": "694d27cf-a24f-4d3e-b5cb-201191a00c33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.19.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.3)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.13.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnx-1.19.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m117.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, onnx, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.19.0 onnxruntime-1.22.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 10) Kick off training (GPU recommended)\n",
        "# Choose training time lengths (crop/pad per sample)\n",
        "# For keystrokes (30s windows), per-event granularity varies; 128 steps is reasonable.\n",
        "# For mouse (6s windows), 128 steps also OK (we pad/crop).\n",
        "KS_TLEN    = 128\n",
        "MOUSE_TLEN = 128\n",
        "\n",
        "# Keystroke channels: [dwell, inter-key-gap] -> infer from one sample\n",
        "ks_in_ch = ks_seqs[0].shape[1] if ks_seqs else 2\n",
        "# Mouse channels: [dx, dy, dt, speed, accel, jerk]\n",
        "mouse_in_ch = mouse_seqs[0].shape[1] if mouse_seqs else 6\n",
        "\n",
        "# Modality-specific hyperparams\n",
        "EPOCHS_KS    = 10\n",
        "EPOCHS_MOUSE = 8\n",
        "BATCH        = 256 if device.type == 'cuda' else 64    # smaller if CPU\n",
        "LR           = 1e-3\n",
        "\n",
        "# Paths\n",
        "KS_CKPT   = ART/'encoder_keystroke.pt'\n",
        "MOUSE_CKPT= ART/'encoder_mouse.pt'\n",
        "KS_ONNX   = ART/'encoder_keystroke.onnx'\n",
        "MOUSE_ONNX= ART/'encoder_mouse.onnx'\n",
        "\n",
        "print(\"Training keystroke encoder...\")\n",
        "ks_model = train_encoder(\n",
        "    ks_seqs, ks_mean, ks_std, ks_in_ch, KS_TLEN,\n",
        "    batch_size=BATCH, epochs=EPOCHS_KS, lr=LR, emb_dim=64, hidden=128,\n",
        "    aug_strength=1.0, ckpt_path=KS_CKPT\n",
        ")\n",
        "\n",
        "print(\"Training mouse encoder...\")\n",
        "mouse_model = train_encoder(\n",
        "    mouse_seqs, mouse_mean, mouse_std, mouse_in_ch, MOUSE_TLEN,\n",
        "    batch_size=BATCH, epochs=EPOCHS_MOUSE, lr=LR, emb_dim=64, hidden=128,\n",
        "    aug_strength=1.0, ckpt_path=MOUSE_CKPT\n",
        ")\n",
        "\n",
        "# Export ONNX\n",
        "export_onnx(ks_model, ks_in_ch, KS_TLEN, KS_ONNX)\n",
        "export_onnx(mouse_model, mouse_in_ch, MOUSE_TLEN, MOUSE_ONNX)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 928
        },
        "id": "Cl4hdhBS1GZi",
        "outputId": "a95bbb90-0cc8-4f94-c28c-2e315b2bd58d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training keystroke encoder...\n",
            "Epoch 01 | loss 2.3510\n",
            "Epoch 02 | loss 2.1202\n",
            "Epoch 03 | loss 2.1001\n",
            "Epoch 04 | loss 2.1140\n",
            "Epoch 05 | loss 2.0622\n",
            "Epoch 06 | loss 2.0391\n",
            "Epoch 07 | loss 2.0069\n",
            "Epoch 08 | loss 1.9868\n",
            "Epoch 09 | loss 1.9609\n",
            "Epoch 10 | loss 1.9477\n",
            "Training mouse encoder...\n",
            "Epoch 01 | loss 2.0295\n",
            "Epoch 02 | loss 1.8002\n",
            "Epoch 03 | loss 1.7680\n",
            "Epoch 04 | loss 1.7463\n",
            "Epoch 05 | loss 1.7290\n",
            "Epoch 06 | loss 1.7199\n",
            "Epoch 07 | loss 1.7585\n",
            "Epoch 08 | loss 1.7168\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OnnxExporterError",
          "evalue": "Module onnx is not installed!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/onnx_proto_utils.py\u001b[0m in \u001b[0;36m_add_onnxscript_fn\u001b[0;34m(model_bytes, custom_opsets)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'onnx'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOnnxExporterError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1358966177.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Export ONNX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mexport_onnx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mks_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mks_in_ch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKS_TLEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKS_ONNX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mexport_onnx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmouse_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmouse_in_ch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMOUSE_TLEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMOUSE_ONNX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-41106151.py\u001b[0m in \u001b[0;36mexport_onnx\u001b[0;34m(model, in_ch, out_len, onnx_path)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mdummy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_ch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     torch.onnx.export(\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monnx_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0minput_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'z'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/_beartype.py\u001b[0m in \u001b[0;36m_coerce_beartype_exceptions_to_warnings\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_coerce_beartype_exceptions_to_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mbeartyped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0m_roar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBeartypeCallHintParamViolation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;31m# Fall back to the original function if the beartype hint is violated.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<@beartype(torch.onnx.utils.export) at 0x7f12a108ba60>\u001b[0m in \u001b[0;36mexport\u001b[0;34m(__beartype_object_139717650025472, __beartype_get_violation, __beartype_conf, __beartype_object_626284128, __beartype_object_139717988060992, __beartype_object_702159072, __beartype_object_10560384, __beartype_object_619397104, __beartype_getrandbits, __beartype_object_702152224, __beartype_object_139717675007616, __beartype_object_619391120, __beartype_object_139718305281600, __beartype_object_619385168, __beartype_object_706497184, __beartype_check_meta, __beartype_func, *args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \"\"\"\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m     _export(\n\u001b[0m\u001b[1;32m    517\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/_beartype.py\u001b[0m in \u001b[0;36m_coerce_beartype_exceptions_to_warnings\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_coerce_beartype_exceptions_to_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mbeartyped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0m_roar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBeartypeCallHintParamViolation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;31m# Fall back to the original function if the beartype hint is violated.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1684\u001b[0m                 )\n\u001b[1;32m   1685\u001b[0m             \u001b[0;31m# insert function_proto into model_proto.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1686\u001b[0;31m             proto = onnx_proto_utils._add_onnxscript_fn(\n\u001b[0m\u001b[1;32m   1687\u001b[0m                 \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1688\u001b[0m                 \u001b[0mcustom_opsets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/_beartype.py\u001b[0m in \u001b[0;36m_coerce_beartype_exceptions_to_warnings\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_coerce_beartype_exceptions_to_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mbeartyped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0m_roar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBeartypeCallHintParamViolation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;31m# Fall back to the original function if the beartype hint is violated.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<@beartype(torch.onnx._internal.onnx_proto_utils._add_onnxscript_fn) at 0x7f12a108ab60>\u001b[0m in \u001b[0;36m_add_onnxscript_fn\u001b[0;34m(__beartype_get_violation, __beartype_conf, __beartype_object_619391120, __beartype_check_meta, __beartype_func, *args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/onnx_proto_utils.py\u001b[0m in \u001b[0;36m_add_onnxscript_fn\u001b[0;34m(model_bytes, custom_opsets)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOnnxExporterError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Module onnx is not installed!\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;31m# For > 2GB model, onnx.load_fromstring would fail. However, because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOnnxExporterError\u001b[0m: Module onnx is not installed!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(mouse_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neO7ypgZvz6y",
        "outputId": "83db78ac-72a2-4a6a-e79b-4c1896771e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TCNEncoder(\n",
            "  (net): Sequential(\n",
            "    (0): Conv1d(6, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "    (7): ReLU(inplace=True)\n",
            "  )\n",
            "  (pool): AdaptiveAvgPool1d(output_size=1)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "export_onnx(ks_model, ks_in_ch, KS_TLEN, KS_ONNX)\n",
        "export_onnx(mouse_model, mouse_in_ch, MOUSE_TLEN, MOUSE_ONNX)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IFE4F42vnvy",
        "outputId": "cba1e1f7-9b19-41ab-d5da-a4f984f35304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exported: /content/drive/MyDrive/mindease_behavior/artifacts/encoder_keystroke.onnx\n",
            "Exported: /content/drive/MyDrive/mindease_behavior/artifacts/encoder_mouse.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 11) Save everything needed for inference\n",
        "BUNDLE = ART/'foundation_bundle.npz'\n",
        "np.savez(\n",
        "    BUNDLE,\n",
        "    ks_mean=ks_mean, ks_std=ks_std, mouse_mean=mouse_mean, mouse_std=mouse_std,\n",
        "    ks_tlen=np.array([KS_TLEN], dtype=np.int32),\n",
        "    mouse_tlen=np.array([MOUSE_TLEN], dtype=np.int32),\n",
        "    ks_in_ch=np.array([ks_in_ch], dtype=np.int32),\n",
        "    mouse_in_ch=np.array([mouse_in_ch], dtype=np.int32),\n",
        ")\n",
        "print(\"Saved bundle:\", BUNDLE)\n",
        "print(\"Artifacts:\")\n",
        "print(\" -\", KS_CKPT)\n",
        "print(\" -\", MOUSE_CKPT)\n",
        "print(\" -\", KS_ONNX)\n",
        "print(\" -\", MOUSE_ONNX)"
      ],
      "metadata": {
        "id": "n3TE20WU1JKt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1176ea05-2492-4022-df24-76fdf87bc82a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved bundle: /content/drive/MyDrive/mindease_behavior/artifacts/foundation_bundle.npz\n",
            "Artifacts:\n",
            " - /content/drive/MyDrive/mindease_behavior/artifacts/encoder_keystroke.pt\n",
            " - /content/drive/MyDrive/mindease_behavior/artifacts/encoder_mouse.pt\n",
            " - /content/drive/MyDrive/mindease_behavior/artifacts/encoder_keystroke.onnx\n",
            " - /content/drive/MyDrive/mindease_behavior/artifacts/encoder_mouse.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If you have a GPU runtime in Colab, use the GPU build of onnxruntime.\n",
        "import sys, subprocess, torch\n",
        "\n",
        "has_cuda = torch.cuda.is_available()\n",
        "print(\"CUDA available:\", has_cuda)\n",
        "\n",
        "def pip(cmd):\n",
        "    print(\">>\", cmd)\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\"] + cmd.split())\n",
        "\n",
        "try:\n",
        "    import onnxruntime as ort\n",
        "    print(\"onnxruntime version:\", ort.__version__)\n",
        "except Exception:\n",
        "    if has_cuda:\n",
        "        pip(\"uninstall -y onnxruntime\")\n",
        "        pip(\"install -U onnxruntime-gpu onnx\")\n",
        "    else:\n",
        "        pip(\"install -U onnx onnxruntime\")\n",
        "    import onnxruntime as ort\n",
        "    print(\"onnxruntime version:\", ort.__version__)\n",
        "\n",
        "# Show providers actually available\n",
        "import onnxruntime as ort\n",
        "sess = ort.InferenceSession\n",
        "print(\"Available providers:\", ort.get_available_providers())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-t5d0HmyqLR",
        "outputId": "f5783905-6395-48c7-f2c8-c122a000f1a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "onnxruntime version: 1.22.1\n",
            "Available providers: ['AzureExecutionProvider', 'CPUExecutionProvider']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze encoders and generate embeddings\n",
        "import os, math, numpy as np\n",
        "from pathlib import Path\n",
        "import onnxruntime as ort\n",
        "\n",
        "DRIVE_ROOT = Path('/content/drive/MyDrive/mindease_behavior')\n",
        "INTER = DRIVE_ROOT/'intermediate'\n",
        "ART   = DRIVE_ROOT/'artifacts'\n",
        "ART.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Paths to encoders and stats\n",
        "KS_ONNX   = ART/'encoder_keystroke.onnx'\n",
        "MOUSE_ONNX= ART/'encoder_mouse.onnx'\n",
        "STATS_NPZ = ART/'norm_stats.npz'\n",
        "\n",
        "assert KS_ONNX.exists(), f\"Missing {KS_ONNX}\"\n",
        "assert MOUSE_ONNX.exists(), f\"Missing {MOUSE_ONNX}\"\n",
        "assert STATS_NPZ.exists(), f\"Missing {STATS_NPZ}\"\n",
        "\n",
        "# Load normalization\n",
        "stats = np.load(STATS_NPZ, allow_pickle=True)\n",
        "ks_mean, ks_std     = stats['ks_mean'], stats['ks_std']\n",
        "mouse_mean, mouse_std = stats['mouse_mean'], stats['mouse_std']\n",
        "print(\"KS mean/std shapes:\", None if ks_mean is None else ks_mean.shape, None if ks_std is None else ks_std.shape)\n",
        "print(\"Mouse mean/std shapes:\", None if mouse_mean is None else mouse_mean.shape, None if mouse_std is None else mouse_std.shape)\n",
        "\n",
        "# Robust NPZ loaders (match your saved formats)\n",
        "def load_keystroke_windows(npz_path: Path, min_len=4):\n",
        "    arr = np.load(npz_path, allow_pickle=True)['windows']\n",
        "    seqs, metas = [], []\n",
        "    # rows: ['user','session', t0, t1, seq]\n",
        "    for row in arr:\n",
        "        row = row.tolist() if isinstance(row, np.ndarray) and row.dtype==object else row\n",
        "        if isinstance(row, (list, tuple)) and len(row) >= 5:\n",
        "            user, sess, t0, t1, seq = row[0], row[1], float(row[2]), float(row[3]), np.asarray(row[-1])\n",
        "            if seq.ndim==2 and seq.shape[0] >= min_len:\n",
        "                seqs.append(seq.astype(np.float32))\n",
        "                metas.append((str(user), str(sess), t0, t1))\n",
        "    return seqs, metas\n",
        "\n",
        "def load_mouse_windows(npz_path: Path, min_len=4):\n",
        "    arr = np.load(npz_path, allow_pickle=True)['windows']\n",
        "    seqs, metas = [], []\n",
        "    # rows: ['user', t0, t1, seq]  (no session)\n",
        "    for row in arr:\n",
        "        row = row.tolist() if isinstance(row, np.ndarray) and row.dtype==object else row\n",
        "        if isinstance(row, (list, tuple)) and len(row) >= 4:\n",
        "            user, t0, t1, seq = row[0], float(row[1]), float(row[2]), np.asarray(row[-1])\n",
        "            if seq.ndim==2 and seq.shape[0] >= min_len:\n",
        "                seqs.append(seq.astype(np.float32))\n",
        "                metas.append((str(user), t0, t1))\n",
        "    return seqs, metas\n",
        "\n",
        "KS_WIN = INTER/'keystroke_windows.npz'\n",
        "MOUSE_WIN = INTER/'mouse_windows.npz'\n",
        "ks_seqs, ks_meta = load_keystroke_windows(KS_WIN)\n",
        "mouse_seqs, mouse_meta = load_mouse_windows(MOUSE_WIN)\n",
        "print(f\"Loaded: keystroke={len(ks_seqs):,}  mouse={len(mouse_seqs):,}\")\n",
        "\n",
        "# Choose providers\n",
        "providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if 'CUDAExecutionProvider' in ort.get_available_providers() else ['CPUExecutionProvider']\n",
        "print(\"Using providers:\", providers)\n",
        "\n",
        "# Create ORT sessions\n",
        "ks_sess    = ort.InferenceSession(str(KS_ONNX), providers=providers)\n",
        "mouse_sess = ort.InferenceSession(str(MOUSE_ONNX), providers=providers)\n",
        "\n",
        "# Our ONNX expects input named 'x' of shape [B, C, T]; it outputs 'z' of shape [B, D]\n",
        "ks_input_name    = ks_sess.get_inputs()[0].name\n",
        "mouse_input_name = mouse_sess.get_inputs()[0].name\n",
        "ks_output_name   = ks_sess.get_outputs()[0].name\n",
        "mouse_output_name= mouse_sess.get_outputs()[0].name\n",
        "\n",
        "# Helper: crop/pad variable length to fixed T for batching\n",
        "def crop_pad_to_len(x, out_len):\n",
        "    T, C = x.shape\n",
        "    if T == out_len:\n",
        "        return x\n",
        "    if T > out_len:\n",
        "        start = np.random.randint(0, T - out_len + 1)\n",
        "        return x[start:start+out_len]\n",
        "    # pad at end\n",
        "    pad = np.zeros((out_len - T, C), dtype=x.dtype)\n",
        "    return np.concatenate([x, pad], axis=0)\n",
        "\n",
        "# If your ONNX was exported with dynamic time axis, you *can* feed varying T,\n",
        "# but batching requires same T per batch. We'll use a fixed TLEN for speed.\n",
        "KS_TLEN    = 128\n",
        "MOUSE_TLEN = 128\n",
        "BATCH = 1024  # adjust if you see OOM\n",
        "\n",
        "def normalize(x, mean, std):\n",
        "    if mean is None or std is None:\n",
        "        return x\n",
        "    return (x - mean) / std\n",
        "\n",
        "def embed_all(seqs, sess, input_name, output_name, mean, std, out_len, batch, tag=\"\"):\n",
        "    N = len(seqs)\n",
        "    D = None\n",
        "    embs = []\n",
        "    for i in range(0, N, batch):\n",
        "        chunk = seqs[i:i+batch]\n",
        "        # crop/pad + normalize + reshape to [B, C, T]\n",
        "        X = []\n",
        "        for s in chunk:\n",
        "            s2 = crop_pad_to_len(s, out_len)\n",
        "            s2 = normalize(s2, mean, std)\n",
        "            X.append(s2.T)  # (C,T)\n",
        "        X = np.stack(X, axis=0).astype(np.float32)  # (B,C,T)\n",
        "        z = sess.run([output_name], {input_name: X})[0]  # (B,D)\n",
        "        if D is None: D = z.shape[1]\n",
        "        embs.append(z.astype(np.float32))\n",
        "        if ((i//batch) % 50)==0:\n",
        "            print(f\"[{tag}] {i+len(chunk):,}/{N:,}\")\n",
        "    Z = np.concatenate(embs, axis=0)\n",
        "    return Z  # (N,D)\n",
        "\n",
        "# ---- Keystroke embeddings ----\n",
        "ks_Z = embed_all(ks_seqs, ks_sess, ks_input_name, ks_output_name,\n",
        "                 ks_mean, ks_std, KS_TLEN, BATCH, tag=\"KS\")\n",
        "print(\"Keystroke embeddings:\", ks_Z.shape)\n",
        "\n",
        "# Save\n",
        "KS_EMB = ART/'embeddings_keystroke.npz'\n",
        "np.savez_compressed(\n",
        "    KS_EMB,\n",
        "    user=np.array([u for (u, s, t0, t1) in ks_meta], dtype=object),\n",
        "    session=np.array([s for (u, s, t0, t1) in ks_meta], dtype=object),\n",
        "    t0=np.array([t0 for (u, s, t0, t1) in ks_meta], dtype=np.float64),\n",
        "    t1=np.array([t1 for (u, s, t0, t1) in ks_meta], dtype=np.float64),\n",
        "    z=ks_Z.astype(np.float32),\n",
        "    allow_pickle=True\n",
        ")\n",
        "print(\"Saved:\", KS_EMB)\n",
        "\n",
        "# ---- Mouse embeddings (per 6s window) ----\n",
        "mouse_Z = embed_all(mouse_seqs, mouse_sess, mouse_input_name, mouse_output_name,\n",
        "                    mouse_mean, mouse_std, MOUSE_TLEN, BATCH, tag=\"MOUSE\")\n",
        "print(\"Mouse embeddings:\", mouse_Z.shape)\n",
        "\n",
        "MOUSE_EMB = ART/'embeddings_mouse.npz'\n",
        "np.savez_compressed(\n",
        "    MOUSE_EMB,\n",
        "    user=np.array([u for (u, t0, t1) in mouse_meta], dtype=object),\n",
        "    t0=np.array([t0 for (u, t0, t1) in mouse_meta], dtype=np.float64),\n",
        "    t1=np.array([t1 for (u, t0, t1) in mouse_meta], dtype=np.float64),\n",
        "    z=mouse_Z.astype(np.float32),\n",
        "    allow_pickle=True\n",
        ")\n",
        "print(\"Saved:\", MOUSE_EMB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wDs4n46yrf7",
        "outputId": "6f4e51d7-40d0-4197-f56e-38c4e64bd8ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KS mean/std shapes: (2,) (2,)\n",
            "Mouse mean/std shapes: (6,) (6,)\n",
            "Loaded: keystroke=172,053  mouse=215,980\n",
            "Using providers: ['CPUExecutionProvider']\n",
            "[KS] 1,024/172,053\n",
            "[KS] 52,224/172,053\n",
            "[KS] 103,424/172,053\n",
            "[KS] 154,624/172,053\n",
            "Keystroke embeddings: (172053, 64)\n",
            "Saved: /content/drive/MyDrive/mindease_behavior/artifacts/embeddings_keystroke.npz\n",
            "[MOUSE] 1,024/215,980\n",
            "[MOUSE] 52,224/215,980\n",
            "[MOUSE] 103,424/215,980\n",
            "[MOUSE] 154,624/215,980\n",
            "[MOUSE] 205,824/215,980\n",
            "Mouse embeddings: (215980, 64)\n",
            "Saved: /content/drive/MyDrive/mindease_behavior/artifacts/embeddings_mouse.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pool 5 consecutive mouse windows per user to approximate 30s (6s * 5)\n",
        "from collections import defaultdict\n",
        "\n",
        "MOUSE_EMB = ART/'embeddings_mouse.npz'\n",
        "me = np.load(MOUSE_EMB, allow_pickle=True)\n",
        "u, t0, t1, z = me['user'], me['t0'], me['t1'], me['z']  # z: [M, 64]\n",
        "\n",
        "by_user = defaultdict(list)\n",
        "for i in range(len(u)):\n",
        "    by_user[str(u[i])].append((float(t0[i]), float(t1[i]), z[i]))\n",
        "\n",
        "pooled_user, pooled_t0, pooled_t1, pooled_z = [], [], [], []\n",
        "K = 5  # group size (5*6s ≈ 30s)\n",
        "for uid, lst in by_user.items():\n",
        "    lst.sort(key=lambda x: x[0])  # sort by start time\n",
        "    # non-overlapping groups of 5\n",
        "    for i in range(0, len(lst)-K+1, K):\n",
        "        seg = lst[i:i+K]\n",
        "        zs = np.stack([e[2] for e in seg], axis=0)  # (K,64)\n",
        "        pooled_user.append(uid)\n",
        "        pooled_t0.append(seg[0][0])\n",
        "        pooled_t1.append(seg[-1][1])\n",
        "        pooled_z.append(zs.mean(axis=0))  # mean pool\n",
        "\n",
        "pooled_z = np.stack(pooled_z, axis=0).astype(np.float32)\n",
        "MOUSE_EMB_30 = ART/'embeddings_mouse_pooled30.npz'\n",
        "np.savez_compressed(\n",
        "    MOUSE_EMB_30,\n",
        "    user=np.array(pooled_user, dtype=object),\n",
        "    t0=np.array(pooled_t0, dtype=np.float64),\n",
        "    t1=np.array(pooled_t1, dtype=np.float64),\n",
        "    z=pooled_z,\n",
        "    allow_pickle=True\n",
        ")\n",
        "print(\"Saved pooled mouse 30s embeddings:\", MOUSE_EMB_30, pooled_z.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urGNdIrGzajU",
        "outputId": "0aa6b84a-a16d-4ad6-bfe6-8ce26a6739d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved pooled mouse 30s embeddings: /content/drive/MyDrive/mindease_behavior/artifacts/embeddings_mouse_pooled30.npz (43192, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ============================\n",
        "# Stage B: Global head (weak labels from face CSV)\n",
        "# ============================"
      ],
      "metadata": {
        "id": "Cd0PdlkCw1Io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, math, json, time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_recall_fscore_support, brier_score_loss\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib"
      ],
      "metadata": {
        "id": "Jh34HJDLw2j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DRIVE_ROOT = Path('/content/drive/MyDrive/mindease_behavior')\n",
        "INTER = DRIVE_ROOT/'intermediate'\n",
        "ART   = DRIVE_ROOT/'artifacts'\n",
        "LAB   = DRIVE_ROOT/'labels'\n",
        "ART.mkdir(parents=True, exist_ok=True)\n",
        "LAB.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---- Inputs (edit path if needed)\n",
        "FACE_CSV      = LAB/'face_stress_30s.csv'     # upload your CSV here in Drive\n",
        "KS_EMB_NPZ    = ART/'embeddings_keystroke.npz'           # from Stage A freeze step\n",
        "MOUSE30_NPZ   = ART/'embeddings_mouse_pooled30.npz'      # optional\n",
        "\n",
        "# ---- Hyperparams / policies\n",
        "CONF_COVERAGE_MIN = 0.30     # use only windows with good face coverage\n",
        "Y_BIN_LOW  = 0.30            # confident \"no-stress\" if stress_prob <= 0.30\n",
        "Y_BIN_HIGH = 0.70            # confident \"stress\"    if stress_prob >= 0.70\n",
        "VAL_SIZE   = 0.25            # val split (small data → simple split)\n",
        "RAND_SEED  = 42\n",
        "USE_MOUSE  = MOUSE30_NPZ.exists()  # fuse if pooled mouse exists"
      ],
      "metadata": {
        "id": "3SIK_AsFw4wX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# B1) Load & prep label CSV\n",
        "# ============================\n",
        "def load_face_csv(csv_path: Path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    # normalize expected columns\n",
        "    # expect at least: t0_unix, t1_unix, stress_prob [, confident, coverage]\n",
        "    # add user_id if missing\n",
        "    if 'user_id' not in df.columns:\n",
        "        df['user_id'] = 'harsh'\n",
        "    # put columns in known names\n",
        "    rename_map = {}\n",
        "    for c in df.columns:\n",
        "        lc = c.strip().lower()\n",
        "        if lc in ('t0', 't0_unix', 'start', 'start_time'):\n",
        "            rename_map[c] = 't0_unix'\n",
        "        elif lc in ('t1', 't1_unix', 'end', 'end_time'):\n",
        "            rename_map[c] = 't1_unix'\n",
        "        elif lc in ('stress', 'stress_prob', 'p_stress'):\n",
        "            rename_map[c] = 'stress_prob'\n",
        "        elif lc in ('conf', 'confident'):\n",
        "            rename_map[c] = 'confident'\n",
        "        elif lc in ('cov', 'coverage', 'face_coverage'):\n",
        "            rename_map[c] = 'coverage'\n",
        "        elif lc in ('user', 'user_id'):\n",
        "            rename_map[c] = 'user_id'\n",
        "        elif lc in ('session', 'session_id'):\n",
        "            rename_map[c] = 'session_id'\n",
        "    df = df.rename(columns=rename_map)\n",
        "\n",
        "    # defaults\n",
        "    if 'confident' not in df.columns:\n",
        "        df['confident'] = 1\n",
        "    if 'coverage' not in df.columns:\n",
        "        df['coverage'] = 1.0\n",
        "    if 'session_id' not in df.columns:\n",
        "        # single-session default\n",
        "        df['session_id'] = 's1'\n",
        "\n",
        "    # types\n",
        "    for col in ['t0_unix','t1_unix','stress_prob','coverage']:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "    df['confident'] = df['confident'].astype(int)\n",
        "    df['user_id']   = df['user_id'].astype(str)\n",
        "    df['session_id']= df['session_id'].astype(str)\n",
        "\n",
        "    # filter usable rows\n",
        "    df = df.dropna(subset=['t0_unix','t1_unix','stress_prob'])\n",
        "    df = df[(df['confident'] == 1) & (df['coverage'] >= CONF_COVERAGE_MIN)].copy()\n",
        "\n",
        "    # snap times to a 30s grid to ease joins (assuming CSV already logged on 30s cadence)\n",
        "    def snap30(x):\n",
        "        return (np.floor(x/30.0)*30.0).astype(np.float64)\n",
        "    df['t0_unix'] = snap30(df['t0_unix'])\n",
        "    df['t1_unix'] = df['t0_unix'] + 30.0\n",
        "\n",
        "    print(f\"[labels] usable rows: {len(df)} (after confident & coverage filter)\")\n",
        "    return df\n",
        "\n",
        "labels = load_face_csv(FACE_CSV)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvrviqJQw-AA",
        "outputId": "77dcd14d-a944-4bbc-a6ce-eb8171351585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[labels] usable rows: 56 (after confident & coverage filter)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# B2) Load embeddings + join\n",
        "# ============================\n",
        "def load_keystroke_emb(npz_path: Path):\n",
        "    zf = np.load(npz_path, allow_pickle=True)\n",
        "    Z = zf['z']          # [N, D]\n",
        "    users = zf['user']   # object array\n",
        "    sess  = zf['session']\n",
        "    t0    = zf['t0'].astype(np.float64)\n",
        "    t1    = zf['t1'].astype(np.float64)\n",
        "    df = pd.DataFrame({\n",
        "        'user_id': users.astype(str),\n",
        "        'session_id': sess.astype(str),\n",
        "        't0_unix': np.floor(t0/30.0)*30.0,\n",
        "        't1_unix': np.floor(t1/30.0)*30.0,\n",
        "    })\n",
        "    return df, Z\n",
        "\n",
        "def load_mouse30_emb(npz_path: Path):\n",
        "    zf = np.load(npz_path, allow_pickle=True)\n",
        "    Z = zf['z']          # [M, D]\n",
        "    users = zf['user']\n",
        "    t0    = zf['t0'].astype(np.float64)\n",
        "    t1    = zf['t1'].astype(np.float64)\n",
        "    df = pd.DataFrame({\n",
        "        'user_id': users.astype(str),\n",
        "        't0_unix': np.floor(t0/30.0)*30.0,\n",
        "        't1_unix': np.floor(t1/30.0)*30.0,\n",
        "    })\n",
        "    df['session_id'] = 's1'  # mouse pooled had no session; fake one for key consistency\n",
        "    return df, Z\n",
        "\n",
        "assert KS_EMB_NPZ.exists(), f\"Missing embeddings at {KS_EMB_NPZ}\"\n",
        "ks_df, ks_Z = load_keystroke_emb(KS_EMB_NPZ)\n",
        "\n",
        "mouse_df, mouse_Z = None, None\n",
        "if USE_MOUSE and MOUSE30_NPZ.exists():\n",
        "    mouse_df, mouse_Z = load_mouse30_emb(MOUSE30_NPZ)\n",
        "    USE_MOUSE = True\n",
        "else:\n",
        "    USE_MOUSE = False\n",
        "\n",
        "# Keep only user 'harsh' for join (since your CSV will be user_id='harsh')\n",
        "ks_df_h = ks_df[ks_df['user_id'] == 'harsh'].copy()\n",
        "if USE_MOUSE:\n",
        "    mouse_df_h = mouse_df[mouse_df['user_id'] == 'harsh'].copy()\n",
        "\n",
        "# Left join labels -> embeddings on exact (user_id, session_id, t0, t1)\n",
        "joined = labels.merge(ks_df_h, on=['user_id','session_id','t0_unix','t1_unix'], how='inner', suffixes=('','_ks'))\n",
        "if len(joined) == 0:\n",
        "    print(\"\\n[WARNING] No keystroke embeddings matched your face-label rows.\")\n",
        "    print(\"This usually means your keystroke embeddings come from PUBLIC datasets, not your live session.\")\n",
        "    print(\"To proceed, you need to collect your own keystroke windows while logging face, then run the ONNX encoder to get embeddings.\")\n",
        "else:\n",
        "    print(f\"[join] Keystroke matches: {len(joined)}\")\n",
        "\n",
        "if USE_MOUSE:\n",
        "    joined_m = labels.merge(mouse_df_h, on=['user_id','session_id','t0_unix','t1_unix'], how='inner', suffixes=('','_mouse'))\n",
        "    print(f\"[join] Mouse(30s) matches: {len(joined_m)}\")\n",
        "else:\n",
        "    joined_m = None\n",
        "\n",
        "# Build feature matrix X and targets y from joins\n",
        "def build_dataset_from_join(jdf, base_df, Z, z_tag='ks'):\n",
        "    if jdf is None or len(jdf)==0:\n",
        "        return None, None, None\n",
        "    # map each (user_id, session_id, t0, t1) row in jdf to embedding row index in base_df\n",
        "    key_cols = ['user_id','session_id','t0_unix','t1_unix']\n",
        "    base_key = base_df[key_cols].reset_index().rename(columns={'index':'idx'})\n",
        "    j2 = jdf.merge(base_key, on=key_cols, how='left')\n",
        "    # keep valid\n",
        "    j2 = j2.dropna(subset=['idx'])\n",
        "    j2['idx'] = j2['idx'].astype(int)\n",
        "    X = Z[j2['idx'].values]\n",
        "    y_prob = j2['stress_prob'].values.astype(np.float32)\n",
        "    # binary mask for extreme labels\n",
        "    mask_lo = y_prob <= Y_BIN_LOW\n",
        "    mask_hi = y_prob >= Y_BIN_HIGH\n",
        "    mask_bin = mask_lo | mask_hi\n",
        "    y_bin = np.where(mask_hi, 1, 0)[mask_bin]\n",
        "    X_bin = X[mask_bin]\n",
        "    # also keep regression set (all confident rows)\n",
        "    return {\n",
        "        'X_all': X, 'y_prob_all': y_prob,\n",
        "        'X_bin': X_bin, 'y_bin': y_bin,\n",
        "        'mask_bin': mask_bin\n",
        "    }, j2, key_cols\n",
        "\n",
        "ks_ds, ks_join, _ = build_dataset_from_join(joined, ks_df_h, ks_Z, 'ks')\n",
        "if USE_MOUSE:\n",
        "    # For mouse we used fake 'session_id'='s1' for base; align labels too:\n",
        "    labels_mouse = labels.copy()\n",
        "    labels_mouse['session_id'] = 's1'\n",
        "    joined_m = labels_mouse.merge(mouse_df_h, on=['user_id','session_id','t0_unix','t1_unix'], how='inner', suffixes=('','_mouse'))\n",
        "    mouse_ds, mouse_join, _ = build_dataset_from_join(joined_m, mouse_df_h, mouse_Z, 'mouse')\n",
        "else:\n",
        "    mouse_ds, mouse_join = None, None\n",
        "\n",
        "# Fuse features if both available\n",
        "def fuse(ks_ds, mouse_ds):\n",
        "    if ks_ds is None or mouse_ds is None: return None\n",
        "    # Align by exact keys inside the joined DataFrames\n",
        "    common = pd.merge(\n",
        "        ks_join[['user_id','session_id','t0_unix','t1_unix']].assign(idx=np.arange(len(ks_join))),\n",
        "        mouse_join[['user_id','session_id','t0_unix','t1_unix']].assign(jdx=np.arange(len(mouse_join))),\n",
        "        on=['user_id','session_id','t0_unix','t1_unix'], how='inner'\n",
        "    )\n",
        "    if len(common)==0: return None\n",
        "    Xk = ks_ds['X_all'][common['idx'].values]\n",
        "    Xm = mouse_ds['X_all'][common['jdx'].values]\n",
        "    X = np.concatenate([Xk, Xm], axis=1)\n",
        "    y_prob = ks_ds['y_prob_all'][common['idx'].values]  # same y by construction\n",
        "    # binary “extreme” subset\n",
        "    mask_lo = y_prob <= Y_BIN_LOW\n",
        "    mask_hi = y_prob >= Y_BIN_HIGH\n",
        "    mask_bin = mask_lo | mask_hi\n",
        "    X_bin = X[mask_bin]\n",
        "    y_bin = np.where(mask_hi, 1, 0)[mask_bin]\n",
        "    return {'X_all':X, 'y_prob_all':y_prob, 'X_bin':X_bin, 'y_bin':y_bin, 'mask_bin':mask_bin}\n",
        "\n",
        "fused_ds = fuse(ks_ds, mouse_ds) if (ks_ds and mouse_ds) else None\n",
        "\n",
        "# Choose which dataset to train on (preference: fused > ks)\n",
        "train_source = 'ks'\n",
        "ds = ks_ds\n",
        "if fused_ds:\n",
        "    train_source = 'fused'\n",
        "    ds = fused_ds\n",
        "\n",
        "if ds is None or ds['X_all'] is None:\n",
        "    raise SystemExit(\"\\n[STOP] No training data after join. Record keystroke/mouse during face logging and re-run embeddings.\")\n",
        "\n",
        "print(f\"[train] Using source: {train_source}  | samples(all)={ds['X_all'].shape[0]}  | samples(bin)={ds['X_bin'].shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "5Kf-DO_oxEgu",
        "outputId": "d0fdf517-dcf8-4e6d-80fb-591d0fe6e227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[WARNING] No keystroke embeddings matched your face-label rows.\n",
            "This usually means your keystroke embeddings come from PUBLIC datasets, not your live session.\n",
            "To proceed, you need to collect your own keystroke windows while logging face, then run the ONNX encoder to get embeddings.\n",
            "[join] Mouse(30s) matches: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "\n[STOP] No training data after join. Record keystroke/mouse during face logging and re-run embeddings.",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m \n[STOP] No training data after join. Record keystroke/mouse during face logging and re-run embeddings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3f0JfSYoxV4s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}